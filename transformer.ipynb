{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b11f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer):\n",
    "    vocab=tokenizer.vocab\n",
    "    special_tokens=tokenizer.special_tokens\n",
    "    return len(vocab)+len(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d504f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL ARE HYPER PARAMETERS. while deploying use mlflow/hyperopt/gridsearch cv\n",
    "import torch\n",
    "from minbpe import BasicTokenizer\n",
    "tokenizer= BasicTokenizer()\n",
    "torch.manual_seed(3647) #to reproduce same model weights + data shuffling\n",
    "block_size=256 #no. of tokens it will process each sequence\n",
    "n_embd=384 #embedding dimension. more dim= can capture more nuanced patterns\n",
    "n_head=6 #better ability to capture diverse relation\n",
    "n_layer=6 #6 is good for mid scaled llm\n",
    "dropout=0.2 # randomly sets 20% of neurons to 0 to overfitting\n",
    "vocab_size=get_vocab_size(tokenizer)\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504f5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size:int):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query=nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value=nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size))) #This matrix will be used to mask future tokens in causal attention.\n",
    "        self.dropout=nn.Dropout(dropout) #prevents overfitting\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _, T, _=x.shape\n",
    "        k=self.key(x)\n",
    "        q=self.query(x)\n",
    "        v=self.value(x)\n",
    "        weights=q@k.transpose(-2,-1)*k.shape[-1]**-0.5\n",
    "        weights=weights.masked_fill(\n",
    "            self.tril[:T,:T]==0,float('-inf'))\n",
    "        weights=F.softmax(weights, dim=1)\n",
    "        weights=self.dropout(weights)\n",
    "        out=weights@v\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads:int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection=nn.Linear(head_size*num_heads, n_embd)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        out=torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out=self.dropout(self.projection(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int):\n",
    "        super().__init__()\n",
    "        head_size=n_embd//n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed_forward=FeedForward(n_embd)\n",
    "        self.layer_norm_1=nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2= nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x=x+self.self_attention(self.layer_norm_1(x))\n",
    "        x=x+self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ae086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table=nn.Embedding(block_size, n_embd) #Since attention is orderless, we need these to make the model understand word order\n",
    "        self.blocks=nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.final_layer_norm=nn.LayerNorm(n_embd)\n",
    "        self.final_linear_layer=nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module: nn.Module): #custom init of weights following normal distribution\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_tokens:torch.Tensor, targets:Optional[torch.Tensor]=None):\n",
    "        B,T=input_tokens.shape #---get embedding from here \n",
    "        token_embedding=self.token_embedding_table(input_tokens)\n",
    "        positional_embedding=self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x=token_embedding+positional_embedding   #get embedding-----to here\n",
    "        x=self.blocks(x)\n",
    "        x=self.final_layer_norm(x)\n",
    "        logits=self.final_linear_layer(x)\n",
    "\n",
    "        if targets  is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,C= logits.shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss=F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input_tokens: torch.Tensor, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            cropped_input=input_tokens[:, -block_size:] #past window size to reffer the appended tokens\n",
    "            logits, _ = self(cropped_input)\n",
    "            logits=logits[:,-1,:]\n",
    "            probs=F.softmax(logits, dim=-1)\n",
    "            idx_next=torch.multinomial(probs, num_samples=1)\n",
    "            input_tokens=torch.cat((input_tokens,idx_next), dim=1)\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c0239d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.935808 M parameters\n"
     ]
    }
   ],
   "source": [
    "model=GPTLanguageModel()\n",
    "model=model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be6fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--token_embedding_table:Embedding (98,304 parameters\n",
      "|--position_embedding_table:Embedding (98,304 parameters\n",
      "|--blocks:Sequential (10,639,872 parameters\n",
      "|   |--0:Block (1,773,312 parameters\n",
      "|   |   |--self_attention:MultiHeadAttention (590,208 parameters\n",
      "|   |   |   |--heads:ModuleList (442,368 parameters\n",
      "|   |   |   |   |--0:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--1:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--2:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--3:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--4:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--5:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |--projection:Linear (147,840 parameters\n",
      "|   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |--feed_forward:FeedForward (1,181,568 parameters\n",
      "|   |   |   |--net:Sequential (1,181,568 parameters\n",
      "|   |   |   |   |--0:Linear (591,360 parameters\n",
      "|   |   |   |   |--1:ReLU (0 parameters\n",
      "|   |   |   |   |--2:Linear (590,208 parameters\n",
      "|   |   |   |   |--3:Dropout (0 parameters\n",
      "|   |   |--layer_norm_1:LayerNorm (768 parameters\n",
      "|   |   |--layer_norm_2:LayerNorm (768 parameters\n",
      "|   |--1:Block (1,773,312 parameters\n",
      "|   |   |--self_attention:MultiHeadAttention (590,208 parameters\n",
      "|   |   |   |--heads:ModuleList (442,368 parameters\n",
      "|   |   |   |   |--0:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--1:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--2:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--3:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--4:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--5:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |--projection:Linear (147,840 parameters\n",
      "|   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |--feed_forward:FeedForward (1,181,568 parameters\n",
      "|   |   |   |--net:Sequential (1,181,568 parameters\n",
      "|   |   |   |   |--0:Linear (591,360 parameters\n",
      "|   |   |   |   |--1:ReLU (0 parameters\n",
      "|   |   |   |   |--2:Linear (590,208 parameters\n",
      "|   |   |   |   |--3:Dropout (0 parameters\n",
      "|   |   |--layer_norm_1:LayerNorm (768 parameters\n",
      "|   |   |--layer_norm_2:LayerNorm (768 parameters\n",
      "|   |--2:Block (1,773,312 parameters\n",
      "|   |   |--self_attention:MultiHeadAttention (590,208 parameters\n",
      "|   |   |   |--heads:ModuleList (442,368 parameters\n",
      "|   |   |   |   |--0:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--1:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--2:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--3:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--4:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--5:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |--projection:Linear (147,840 parameters\n",
      "|   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |--feed_forward:FeedForward (1,181,568 parameters\n",
      "|   |   |   |--net:Sequential (1,181,568 parameters\n",
      "|   |   |   |   |--0:Linear (591,360 parameters\n",
      "|   |   |   |   |--1:ReLU (0 parameters\n",
      "|   |   |   |   |--2:Linear (590,208 parameters\n",
      "|   |   |   |   |--3:Dropout (0 parameters\n",
      "|   |   |--layer_norm_1:LayerNorm (768 parameters\n",
      "|   |   |--layer_norm_2:LayerNorm (768 parameters\n",
      "|   |--3:Block (1,773,312 parameters\n",
      "|   |   |--self_attention:MultiHeadAttention (590,208 parameters\n",
      "|   |   |   |--heads:ModuleList (442,368 parameters\n",
      "|   |   |   |   |--0:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--1:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--2:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--3:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--4:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--5:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |--projection:Linear (147,840 parameters\n",
      "|   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |--feed_forward:FeedForward (1,181,568 parameters\n",
      "|   |   |   |--net:Sequential (1,181,568 parameters\n",
      "|   |   |   |   |--0:Linear (591,360 parameters\n",
      "|   |   |   |   |--1:ReLU (0 parameters\n",
      "|   |   |   |   |--2:Linear (590,208 parameters\n",
      "|   |   |   |   |--3:Dropout (0 parameters\n",
      "|   |   |--layer_norm_1:LayerNorm (768 parameters\n",
      "|   |   |--layer_norm_2:LayerNorm (768 parameters\n",
      "|   |--4:Block (1,773,312 parameters\n",
      "|   |   |--self_attention:MultiHeadAttention (590,208 parameters\n",
      "|   |   |   |--heads:ModuleList (442,368 parameters\n",
      "|   |   |   |   |--0:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--1:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--2:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--3:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--4:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--5:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |--projection:Linear (147,840 parameters\n",
      "|   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |--feed_forward:FeedForward (1,181,568 parameters\n",
      "|   |   |   |--net:Sequential (1,181,568 parameters\n",
      "|   |   |   |   |--0:Linear (591,360 parameters\n",
      "|   |   |   |   |--1:ReLU (0 parameters\n",
      "|   |   |   |   |--2:Linear (590,208 parameters\n",
      "|   |   |   |   |--3:Dropout (0 parameters\n",
      "|   |   |--layer_norm_1:LayerNorm (768 parameters\n",
      "|   |   |--layer_norm_2:LayerNorm (768 parameters\n",
      "|   |--5:Block (1,773,312 parameters\n",
      "|   |   |--self_attention:MultiHeadAttention (590,208 parameters\n",
      "|   |   |   |--heads:ModuleList (442,368 parameters\n",
      "|   |   |   |   |--0:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--1:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--2:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--3:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--4:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |   |--5:Head (73,728 parameters\n",
      "|   |   |   |   |   |--key:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--query:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--value:Linear (24,576 parameters\n",
      "|   |   |   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |   |--projection:Linear (147,840 parameters\n",
      "|   |   |   |--dropout:Dropout (0 parameters\n",
      "|   |   |--feed_forward:FeedForward (1,181,568 parameters\n",
      "|   |   |   |--net:Sequential (1,181,568 parameters\n",
      "|   |   |   |   |--0:Linear (591,360 parameters\n",
      "|   |   |   |   |--1:ReLU (0 parameters\n",
      "|   |   |   |   |--2:Linear (590,208 parameters\n",
      "|   |   |   |   |--3:Dropout (0 parameters\n",
      "|   |   |--layer_norm_1:LayerNorm (768 parameters\n",
      "|   |   |--layer_norm_2:LayerNorm (768 parameters\n",
      "|--final_layer_norm:LayerNorm (768 parameters\n",
      "|--final_linear_layer:Linear (98,560 parameters\n"
     ]
    }
   ],
   "source": [
    "def print_model_structure(model: torch.nn.Module, indent: str=''):\n",
    "    for name, child, in model.named_children():\n",
    "        params=sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{indent}|--{name}:{child.__class__.__name__} ({params:,} parameters\")\n",
    "        print_model_structure(child, indent+'|   ')\n",
    "\n",
    "print_model_structure(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
